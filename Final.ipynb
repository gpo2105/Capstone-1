{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ff9f24",
   "metadata": {},
   "source": [
    "# Project Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9d48c",
   "metadata": {},
   "source": [
    "__[Department of Transportation's Responsibilities](https://www.google.com/search?client=safari&rls=en&q=department+of+transportation+responsibilities&ie=UTF-8&oe=UTF-8)__\n",
    "\n",
    "Through this dataset, I have identified the three following patterns:\n",
    "1. IDEA 1 w/ LOCAL LINK\n",
    "2. IDEA 2 w/ LOCAL LINK\n",
    "3. IDEA 3 w/ LOCAL LINK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ce2a8",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "\n",
    "- __[Kaggle-Full](https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents/data)__\n",
    "\n",
    "- __[Kaggle-Sampled](https://drive.google.com/file/d/1U3u8QYzLjnEaSurtZfSAS_oh9AT2Mn8X/edit)__\n",
    "\n",
    "- __[Bing API](https://learn.microsoft.com/en-us/bingmaps/rest-services/traffic/get-traffic-incidents#supported-http-methods)__\n",
    "\n",
    "- __[MQuest API](https://developer.mapquest.com/documentation/api/traffic/incidents/get.html)__\n",
    "\n",
    "- __['A Countrywide Traffic Accident Dataset'](https://arxiv.org/pdf/1906.05409)__\n",
    "\n",
    "- __['Accident Risk Prediction based on Heterogenous Sparse Data: New Dataset & Insights](https://arxiv.org/pdf/1909.09638)__\n",
    "\n",
    "- __['Census Data'](https://www.census.gov/acs/www/data/data-tables-and-tools/subject-tables/)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe8832b",
   "metadata": {},
   "source": [
    "# 1-Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a277507",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf39de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilities\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Data Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msgno\n",
    "\n",
    "#Time Data\n",
    "from datetime import date, timedelta\n",
    "\n",
    "\n",
    "#PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#Visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy.stats import zscore, f_oneway, chi2_contingency, chisquare\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "# Spatial Tools\n",
    "import geopy\n",
    "import geopy.distance\n",
    "from geopy.geocoders import Nominatim\n",
    "import census\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25561598",
   "metadata": {},
   "source": [
    "## Info on New Libraries\n",
    "For improved analysis, these libraries were included, but not covered in the course material:\n",
    "\n",
    "- __[Geopy:  ](https://geopy.readthedocs.io/en/stable/)__\n",
    "\n",
    "- Census:\n",
    "    * __[PyPi](https://pypi.org/project/census/)__\n",
    "    * __[API](https://www.census.gov/data/developers/guidance/api-user-guide.html)__\n",
    "\n",
    "- MissingNo:  \n",
    "    * __[Library](https://github.com/ResidentMario/missingno)__\n",
    "    \n",
    "    * __[Tutorial](https://www.geeksforgeeks.org/python-visualize-missing-values-nan-values-using-missingno-library/)__\n",
    "\n",
    "- __[Yellowbrick](https://www.scikit-yb.org/en/latest/)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acc8401",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8091e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.width', 500)\n",
    "_LITE_SWITCH_ = False\n",
    "_SPARK_ = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea05ea",
   "metadata": {},
   "source": [
    "## Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9546653",
   "metadata": {},
   "source": [
    "### For Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db1326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def census_data(yr,state_county=False):\n",
    "    base_link = f'https://api.census.gov/data/{yr}/acs/acs1?get=group(B08301)&ucgid=pseudo(0100000US$0400000)'\n",
    "    #base_link = f'https://api.census.gov/data/{yr}/acs/acs5/subject?'\n",
    "    if state_county:\n",
    "        add_on = f'get=group(B08301)&POPGROUP=001&ucgid=pseudo(0100000US$0500000)'\n",
    "    else:\n",
    "        add_on = f''\n",
    "        #add_on = f'get=group(B08301)&ucgid=pseudo(0100000US$0400000)'\n",
    "\n",
    "    census_link = base_link + add_on\n",
    "    \n",
    "    r = requests.get(census_link)\n",
    "    df = pd.DataFrame(r.json())\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.drop(0)\n",
    "    df = df.set_index('NAME')\n",
    "    columns = [c for c in df.columns if c.endswith('E')]\n",
    "    df = df[columns]\n",
    "    col_names = ['Total','Car_Truck_Van',\n",
    "                'DriveAlone',\n",
    "                'Carpool','2Person_Pool','3Person_Pool','4Person_Pool','56Person_Pool','7UpPerson_Pool',\n",
    "                'Public','Bus','Subway','LongDistanceRail','LightRail','Ferry',\n",
    "                'Taxi','Motorcycle','Bicycle','Walked','Other','WorkFromHome'\n",
    "                ]\n",
    "\n",
    "    df.columns = col_names\n",
    "    df = df.astype(int)\n",
    "    df[col_names] = df[col_names].apply(lambda r:r/r['Total'],axis=1)\n",
    "    columns_filtered = ['DriveAlone','Carpool','Bus','Subway','LongDistanceRail','LightRail',\n",
    "                        'Ferry','Taxi','Motorcycle','Bicycle','Walked','Other','WorkFromHome']\n",
    "    return df[columns_filtered]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a9ad18",
   "metadata": {},
   "source": [
    "### Exploration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf03d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For High-level data exploration\n",
    "def count_outliers(df_col,cap=3):\n",
    "    zs = zscore(df_col)\n",
    "    return df_col[zs > cap].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07002952",
   "metadata": {},
   "source": [
    "### Date-Related Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31cb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holiday Indicators\n",
    "# Given by Google:\n",
    "def calculate_memorial_day(year):\n",
    "    # Get the last day of May\n",
    "    last_day_of_may = date(year, 5, 31)\n",
    "\n",
    "    # Get the weekday of the last day of May (0=Monday, 6=Sunday)\n",
    "    weekday = last_day_of_may.weekday()\n",
    "\n",
    "    # Calculate the offset to get to the last Monday\n",
    "    offset = (weekday - 0) % 7\n",
    "\n",
    "    # Calculate Memorial Day\n",
    "    memorial_day = last_day_of_may - timedelta(days=offset)\n",
    "\n",
    "    return memorial_day\n",
    "\n",
    "# Given by Google:\n",
    "def get_thanksgiving_date(year):\n",
    "    # Get the first day of November\n",
    "    first_november = date(year, 11, 1)\n",
    "    \n",
    "    # Calculate the day of the week for the first of November (0 = Monday, 6 = Sunday)\n",
    "    first_day_weekday = first_november.weekday()\n",
    "    \n",
    "    # Calculate the number of days to add to get to the first Thursday\n",
    "    days_to_first_thursday = (3 - first_day_weekday) % 7\n",
    "    \n",
    "    # Calculate the date of the first Thursday of November\n",
    "    first_thursday = first_november + timedelta(days=days_to_first_thursday)\n",
    "    \n",
    "    # Add 3 weeks (21 days) to get to the fourth Thursday (Thanksgiving)\n",
    "    thanksgiving_date = first_thursday + timedelta(days=21)\n",
    "    \n",
    "    return thanksgiving_date\n",
    "\n",
    "def get_labor_day(year):\n",
    "    first = date(year,9,1)\n",
    "    d = first\n",
    "    while d.weekday() > 0:\n",
    "       d = d + timedelta(days=1)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a0bd9",
   "metadata": {},
   "source": [
    "### Functionality for Running Statistical Analysis & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_category(feature_name,dataset):\n",
    "    # Time of Impact--ANOVA\n",
    "    model_time = ols(f\"Q('Time_of_Impact(hr)') ~ C({feature_name})\", data=dataset).fit()\n",
    "    anova_table_time = sm.stats.anova_lm(model_time, typ=2)\n",
    "    # Distance of Impact--ANOVA\n",
    "    model_distance = ols(f\"Q('Distance(mi)') ~ C({feature_name})\", data=dataset).fit()\n",
    "    anova_table_distance = sm.stats.anova_lm(model_distance, typ=2)\n",
    "    # Severity Distribution--ANOVA\n",
    "    contingency_table = pd.crosstab(dataset[feature_name], dataset['Severity'])\n",
    "    chi_results = chi2_contingency(contingency_table)\n",
    "    return anova_table_time, anova_table_distance, chi_results\n",
    "\n",
    "def display_categories(feature_name,dataset):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    #sns.boxplot(x=feature_name, y='Severity', data=dataset, palette='viridis')\n",
    "    sns.boxplot(hue=feature_name, y='Severity', data=dataset, palette='viridis')\n",
    "    plt.title('Accident Severity by '+feature_name)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    dataset[feature_name].value_counts().plot(kind='bar', color='skyblue')\n",
    "    plt.title('Total Accidents by '+feature_name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed71e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_results(feature_name,dataset,alpha=0.99,visuals=True,reporting=True):\n",
    "    anova_time,anova_distance,chi_test= test_category(feature_name,dataset)\n",
    "    p_vals = dict()\n",
    "    p_vals['time'] = ('ANOVA',anova_time['PR(>F)'].iloc[0])\n",
    "    p_vals['distance'] = ('ANOVA', anova_distance['PR(>F)'].iloc[0])\n",
    "    p_vals['severity'] = ('CHI-Squared', chi_test[1])\n",
    "\n",
    "    test_results = []\n",
    "    for topic, p_val in p_vals.items():\n",
    "        msg = f\"Based on the {p_val[0]} test's p-value of {p_val[1]:.3f}, \"\n",
    "        null_hypo = f\"the null hypothesis that there is no significant difference in incident {topic}\"\n",
    "        null_hypo += f\" across different {feature_name}s.\"\n",
    "        if (1 - alpha) > p_val[1]:\n",
    "            msg += f\"we reject \" + null_hypo\n",
    "        else:\n",
    "            msg += f\"we fail to reject \" + null_hypo\n",
    "        if(reporting): print(msg)\n",
    "        test_results.append(msg)\n",
    "    if visuals:\n",
    "        display_categories(feature_name=feature_name,dataset=dataset)\n",
    "    return test_results, [anova_time,anova_distance,chi_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb23fb",
   "metadata": {},
   "source": [
    "## Load Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4ca8a",
   "metadata": {},
   "source": [
    "### Main Dataset\n",
    "Will depend on settings: lite_switch gives option to use the scaled down dataset.  _SPARK_ will give option of loading with pyspark (which will also determine functionality through out project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24ecb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    spark = SparkSession.builder.appName(\"Accident Data Project\").getOrCreate()\n",
    "    data = spark.read.csv('US_Accidents_March23.csv',header=True,inferSchema=True)\n",
    "else:\n",
    "    if _LITE_SWITCH_:\n",
    "        data = pd.read_csv('US_Accidents_March23_sampled_500k.csv')\n",
    "    else:\n",
    "        data = pd.read_csv('US_Accidents_March23.csv')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692cb090",
   "metadata": {},
   "source": [
    "### Other User-Built Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e7f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "commute_options = ['NAME','Year','DriveAlone','Carpool','Bus','Subway','LongDistanceRail','LightRail','Taxi','Motorcycle','Bicycle','Walked','Other','WorkFromHome']\n",
    "all_commutes = pd.DataFrame(columns=commute_options)\n",
    "for yr in range(2016,2024):\n",
    "    if(os.path.exists(f'Commute_{yr}.csv')):\n",
    "        commute_census = pd.read_csv(f'Commute_{yr}.csv')\n",
    "    else:\n",
    "        try:\n",
    "            commute_census = census_data(yr)\n",
    "        except:\n",
    "            commute_census = census_data(yr-1)\n",
    "        commute_census.to_csv(f'Commute_{yr}.csv')\n",
    "    commute_census['Year'] = yr\n",
    "    \n",
    "    all_commutes = pd.concat([all_commutes,commute_census],axis=0)\n",
    "\n",
    "    all_commutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0603b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "locator = Nominatim(user_agent='Capstone-1')\n",
    "cities = ['NYC','Chicago','Miami','Atlanta','Charlotte','Dallas','Houston','Denver','LA','Seattle']\n",
    "city_locals = pd.DataFrame(index=cities,columns=['lat','long'])\n",
    "for city in cities:\n",
    "    location=locator.geocode(city)\n",
    "    city_locals.loc[city,'lat'] = location.latitude\n",
    "    city_locals.loc[city,'long'] = location.longitude\n",
    "city_locals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362b13e",
   "metadata": {},
   "source": [
    "# 2-Initial EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ff5a66",
   "metadata": {},
   "source": [
    "## Schema & Feature Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    data.printSchema()\n",
    "    print(\"Features: \",len(data.columns))\n",
    "    print(\"Entries:  \",data.count())\n",
    "else:\n",
    "    data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    data.show(5,vertical=False)\n",
    "else:\n",
    "    data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb030318",
   "metadata": {},
   "source": [
    "Very large dataset with over 7.7 million observations.  (I also chose to do some initial analysis ont he kaggle-provided sampled dataset which contains only 500k observations.)  So we will have to handle with care either by using Spark or through other tricks.  \n",
    "\n",
    "Dimensionally much more managable with just 45 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fcfafa",
   "metadata": {},
   "source": [
    "1. Identifiers in ID and Source:  Some of the research treats the Source as an important consideration but for this project's purposes, I think it can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd38944",
   "metadata": {},
   "source": [
    "2. Nature of Incident:\n",
    "- 'Severity' would appear to be the primary variable of interest; 'Description' might be interesting to experiment with.\n",
    "    * Upon further research, there is a 'Distance(mi)' feature that reflects the mileage of road that was impacted.\n",
    "    * Furthermore, we can calculate the duration of impact by calculating the delta between 'Start_Time' & 'End_Time'\n",
    "    * We may be able to also create a feature of impacted area, depending on what we find with 'End_Lat' and 'End_Lng'\n",
    "\n",
    "- 'Description' feature might be an interesting avenue to explore.  However, I suspect that it is collected/generated after the fact so there wouldn't be much predictive value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab1f288",
   "metadata": {},
   "source": [
    "3. A number of location features\n",
    "    * 'Start_Lat'/'Start_Lng' are easy enough to interpret; 'End_Lat' _ 'End_Lng' are there to define the space the accident affects.  However, we already have the 'Distance(mi)'.  \n",
    "\n",
    "    * We have address features: \n",
    "        - City, ZipCode, County and State all have utility that is immediately benefecial.\n",
    "        - Street may be interesting if we can use it to identify types of roads.\n",
    "        - Airport Code may be an interesting way to look at flight traffic vs road incidents.  However, as discussed below, it is unclear what this feature actually represents.\n",
    "        - We can likely get rid of Country feature.\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "- Next are 10 binary features which seem to provide some information about the road infrastructure at the location of the accident.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915aede",
   "metadata": {},
   "source": [
    "4. We have a handful of time-related features: \n",
    "- 'Start_Time' and 'End_Time' need to be [converted to datetimes](#datetimes).  Right now they are just strings and there appears to be an issue with inconsistent formatting.\n",
    "\n",
    "- It may be interesting to use these values to engineer some features like time of day, season, weekday/weekend, etc.\n",
    "\n",
    "- The information contained in 'Weather_Timestamp' is unclear and I do not believe the Timezone data will be worthwhile to retain since the start/end time features are local (according to Kaggle).\n",
    "\n",
    "- There are 5 binary features indicating whether or not it was light or dark out at the time of the incident."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad64046",
   "metadata": {},
   "source": [
    "5. There are 9 (not including 'Weather_Timestamp') features on the weather:\n",
    "- Temp, Wind Chill, Humidity, Pressure, Visibility, Wind Speed, and Precipitation are all continuous features with pretty straight-forward explanations.  They may be useful in a regression analysis.\n",
    "\n",
    "- Weather Condition may be preferable under these circumstances since the most important target feature will be the number of accidents under certain circumstances.  (As disussed above, we cannot create a probability metric without a massive amount of extra data resources.)\n",
    "\n",
    "- HOWEVER, this feature space's cardinality is troubling large and there are major imbalances. So we would need to consolidate:\n",
    "    * We could do this manually by choosing a handful of the most frequent categories and assigning the less frequent categories into one of them.\n",
    "    * But this requires some discretion which may not be preferable.  Instead, we look at the option of clustering the observatins based on the numerical weather features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049c65c7",
   "metadata": {},
   "source": [
    "6. There are 13 binary variables which indicate the presence of a different infrastructure/roadway structure/feature.\n",
    "- Six strucutres are obviously meant to decrease accident frequency (& severity?):\n",
    "    * Stop\n",
    "    * Traffic Signal\n",
    "    * Roundabout\n",
    "    * Turning Loop\n",
    "    * Traffic Calming (?)\n",
    "    * Bump\n",
    "\n",
    "- For three, their presence would intuitively increase the likelihood of accidents:\n",
    "    * Amenity\n",
    "    * Railway\n",
    "    * Station\n",
    "\n",
    "- Without additional info, the other four's intuitive impact is unclear:\n",
    "    * No Exit\n",
    "    * Crossing\n",
    "    * Junction\n",
    "    * Give Way\n",
    "\n",
    "- It may also be informative to understand the interactions of these categories:\n",
    "    * Are some mutually exclusive?\n",
    "    * Do combinations of occurences linearly affect the number of accidents? How about the nature of an accident? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e7def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize features into global variables:\n",
    "_FEATURES_ = data.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33f4633",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_NUMERICS_ = ['Severity','Distance(mi)','Temperature(F)',\n",
    "              'Wind_Chill(F)','Humidity(%)','Pressure(in)',\n",
    "              'Visibility(mi)','Wind_Speed(mph)','Precipitation(in)'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef3b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TARGET_ = ['Severity','Distance(mi)']\n",
    "\n",
    "_INFRASTRUCTURE_ = ['Stop','Traffic_Signal','Roundabout','Turning_Loop',\n",
    "                    'Traffic_Calming','Bump','Amenity','Railway',\n",
    "                    'Station','No_Exit','Crossing','Junction',\n",
    "                    'Give-Way'\n",
    "                    ]\n",
    "\n",
    "_WEATHER_ = ['Weather_Condition','Temperature(F)','Wind_Direction'\n",
    "             'Wind_Chill(F)','Humidity(%)','Pressure(in)',\n",
    "             'Visibility(mi)','Wind_Speed(mph)','Precipitation(in)'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9d0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "_USELESS_ = ['Source','Weather_Timestamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab7f48",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00018eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    missing = data.select(*[F.sum(F.isnull(F.col(c)).cast(\"int\")).alias(c) for c in data.columns])\n",
    "    print(missing.show(vertical=True))\n",
    "else:\n",
    "    print(data.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46636a",
   "metadata": {},
   "source": [
    "There is a lot of missing data.  But let's first point out the positives:\n",
    "- The two primary target variables, Severity & Distance(mi), do not have any missing observations.  \n",
    "\n",
    "- We also have no missing observations of time (Start and End) so we should be able to create the third feature, impact duration, without issue.\n",
    "\n",
    "- With so few observations missing a description, it is hard to imagine just blindly dropping those would impact our analysis.\n",
    "\n",
    "- Some observations are missing address features (Street, City + Zipcode) but again the number is a small fraction (even if there is no overlap) of the total database so blindly dropping would be unlikely to be a concern.  Furthermore, all the observations have county and state features. As a result, we have plenty of options which will depend on the topics we choose to dig deep on.\n",
    "\n",
    "- This leaves us with only a few groups of features which have missing values that requries some consideration before handling:  \n",
    "    * End longitude/latitude\n",
    "    * Weather\n",
    "    * Night/Day\n",
    "\n",
    "But let's first look at how the co-occurence of these missing observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgno.heatmap(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fef6fa",
   "metadata": {},
   "source": [
    "Good to know:\n",
    "* Confirmed that Latitude/Longitude Data are strictly missing together.\n",
    "\n",
    "* Weather data seems to be missing together in general but not perfectly missing and not a uniform relationship (Precipitation and Wind Direction for example).\n",
    "\n",
    "* The day/night variables are strictly missing together.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1615986f",
   "metadata": {},
   "source": [
    "### Ending Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9101f4e",
   "metadata": {},
   "source": [
    "The simplest solution would be to just setting missing end coordinates to the observation's start coordinates.  But is this a common occurence among the observations which none of these variables are missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bbaa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_startVend = data[(data['Start_Lat']==data['End_Lat']) & (data['Start_Lng']==data['End_Lng'])]\n",
    "\n",
    "same_startVend.shape[0]/data['End_Lat'].notnull().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f2f15",
   "metadata": {},
   "source": [
    "5% of the observations which have both start & end coordinates.  So it is not uncommon but not frequent enough to make me comfortable changing half of the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01b5525",
   "metadata": {},
   "source": [
    "We have speculated that there is a relationship between distance feature and the coordinates.  Let's explore that further to inform our decision to handle missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b94968",
   "metadata": {},
   "outputs": [],
   "source": [
    "(same_startVend['Distance(mi)'] > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb4af6",
   "metadata": {},
   "source": [
    "So the distance metric for all of the observations with equal start and end coordinates is 0.\n",
    "\n",
    "Let's consider the same analysis for those observations missing the end coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da06cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ends = data[data['End_Lat'].isnull()]['Distance(mi)']\n",
    "missing_ends.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a6aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "(missing_ends==0).sum() / missing_ends.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd425bb",
   "metadata": {},
   "source": [
    "So for a significant portion of the observations missing the end coordinates, the distance metric is also 0.  \n",
    "\n",
    "What does distance for the rest look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35dea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data['End_Lat'].isnull()) & (data['Distance(mi)'] > 0)]['Distance(mi)'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb4a061",
   "metadata": {},
   "source": [
    "For these observations (especially the large outliers), it does not appear advisable to force the end coordinates to equal the start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c1262",
   "metadata": {},
   "source": [
    "ACTION DECISION:  Set all missing ending coordinates equal to their starting coordinates.  (Possibility that this feature may ultimately be tossed in the future.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c636c39",
   "metadata": {},
   "source": [
    "### Weather\n",
    "Based on some research and intuition, we can make some decisions on how to handle missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954adefb",
   "metadata": {},
   "source": [
    "#### Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Weather_Condition.isnull().sum()/data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a6ac20",
   "metadata": {},
   "source": [
    "It's only 2% of the data and, as it stands, I have no way have extrapolating how any individual observation should be classified.  Therefore, it is probably safe to remove."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f46c2a",
   "metadata": {},
   "source": [
    "ACTION DECISION: Remove all observations missing the Weather Condition feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a577c784",
   "metadata": {},
   "source": [
    "ACTION DECISION: Drop Missing Temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c6067",
   "metadata": {},
   "source": [
    "#### Precipitation, Wind Speed & Wind Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data['Precipitation(in)']==0).sum()/data[data['Precipitation(in)'].notnull()].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c04b5",
   "metadata": {},
   "source": [
    "Most (> 90%) of the observations not missing precipitation have 0. So it doesn't seem unreasonable to set missing values to 0. (It may be worthwhile to check against the weather condition in the future.)\n",
    "\n",
    "ACTION DECISION: Set missing observations of precipitation to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data['Wind_Speed(mph)']==0).sum()/data[data['Wind_Speed(mph)'].notnull()].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5e0d7b",
   "metadata": {},
   "source": [
    "While not as frequent, there are plenty (13%) of observations with wind speed equal to 0.  So it doesn't seem unreasonable to set missing values to 0. (It may be worthwhile to check against the observations wind direction and/or wind chill in the future.)\n",
    "\n",
    "ACTION DECISION: Set missing observations of wind speed to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e7e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Wind_Direction'].value_counts(normalize=True,dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5172023d",
   "metadata": {},
   "source": [
    "A significant portion of the observations have some equivalent of calm, which I interpet it as 0 wind.  So I do not believe it is unreasonable to assume missing values to be the equivalent to 0 wind as well.  (Again, it may be worthwhile to check against the other wind features in the future.)\n",
    "\n",
    "ACTION DECISION: Set missing observtions of wind direction to 'CALM' or some equivalent.  (Will depend on how I end up cleaning the Wind_Direction feature.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48615b31",
   "metadata": {},
   "source": [
    "##### Wind Chill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e253ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Wind_Chill(F)']==data['Temperature(F)']].shape[0]/data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2084d842",
   "metadata": {},
   "source": [
    "Over half of the observations occur where there is no additional effect on temp from the wind, so I do not think it is too dangerous to set missing wind-chill values equal to the temperture. (It may be worthwhile to check against the wind speed in the future.)\n",
    "\n",
    "ACTION DECISION: Set missing values of wind chill to the  observation's temperture value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41aded",
   "metadata": {},
   "source": [
    "#### Pressure, Visibility & Humidity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d75951",
   "metadata": {},
   "source": [
    "Per this google search https://www.google.com/search?client=safari&rls=en&q=can+air+pressure+be+0&ie=UTF-8&oe=UTF-8, air pressure cannot be 0 (except in a vacuum).  So without any anchor value to use, I do not think it wise to assign missing observations to any value.  \n",
    "\n",
    "ACTION DECISION: Remove all observations missing a value for the Pressure feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69612f",
   "metadata": {},
   "source": [
    "There is also no obvious value to use for visibility.\n",
    "\n",
    "ACTION DECISION: Remove all observations missing a value for the Pressure feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5321ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data['Humidity(%)']==0).sum()/data[data['Humidity(%)'].notnull()].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d52699",
   "metadata": {},
   "source": [
    "There are no observations with 0% humidity, so we do not have any reasonable anchor value to use once again.  \n",
    "\n",
    "ACTION DECISION: Remove all observations missing a value for the Humidity feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df9b369",
   "metadata": {},
   "source": [
    "## Duplicates\n",
    "\n",
    "There are no duplicates to deal with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b6cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    duplicates = int(data.count() - data.dropDuplicates().count())\n",
    "    duplicates.show()\n",
    "else:\n",
    "    print(data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d3a473",
   "metadata": {},
   "source": [
    "Nothing to worry about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e90bea6",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefd5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    pass\n",
    "else:\n",
    "    print(pd.DataFrame({c:{z:count_outliers(data[c],z) for z in [3,5,10,15,20]} for c in _NUMERICS_}).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603632b",
   "metadata": {},
   "source": [
    "Distance seems to be the only feature in the original dataset (lite or full) that has significant outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{(data['Distance(mi)'] > 100).sum()} of the observations are greater than 100 miles.  Lets drop these')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc6f652",
   "metadata": {},
   "source": [
    "ACTION DECISION: Drop observations with distance impacted to be over 100 miles.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045b01bf",
   "metadata": {},
   "source": [
    "## High-Level Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bdb837",
   "metadata": {},
   "source": [
    "### Nature of Accident\n",
    "\n",
    "We will eventually create another feature to describe the nature of any accident but for now, we have severity (a discrete variable) and distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e63433",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Severity'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2938a1",
   "metadata": {},
   "source": [
    "We have significant class imbalances.  Even though it is unclear what methodology (if any) is used to determine severity, this imbalance makes sense:  Low-impact incidents are likely underreported and the highest-impact incidents are, fortunately, rare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed4635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Distance(mi)'].value_counts(bins=25,normalize=True).sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832cfb0",
   "metadata": {},
   "source": [
    "We know from exploring this feature's missing values and outliers what the issues for this feature.  The binning makes it seem that there are negative values but this is not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f664c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = f'{(data['Distance(mi)']==0).sum()/data.shape[0]:.2f} of observations have 0 impact distance and '\n",
    "msg += f'{(0 > data['Distance(mi)']).sum()} observations with negative impact distance.'\n",
    "\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e392e",
   "metadata": {},
   "source": [
    "### Time Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d4214d",
   "metadata": {},
   "source": [
    "To explore properly, we need to convert to datetime type (from pandas).  Also, need to convert into seconds and then into hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62352b91",
   "metadata": {},
   "source": [
    "It is required to set the 'format' parameter to \"mixed\".  (Too many observations to understand why / explore observations setting off the error.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e19c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.to_datetime(data['Start_Time'],format='mixed')\n",
    "end = pd.to_datetime(data['End_Time'],format='mixed')\n",
    "time_change = (end - start).astype('timedelta64[s]').dt.seconds/3600\n",
    "time_change.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee07b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_change.value_counts(bins=25,normalize=True).sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd9122",
   "metadata": {},
   "source": [
    "For the most part, this distribution makes sense.  Again despite the binning, there are no negative values for the time of impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b714de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = f'{(time_change==0).sum()} of the observations have 0 impact duration and '\n",
    "msg += f'{(0 > time_change).sum()} observations with negative impact duration.'\n",
    "\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e73e86",
   "metadata": {},
   "source": [
    "It is unusual to see that some observations have no duration but it such a small number and purposely possible that it means essentially 0.  So no need to stess."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10c8a2b",
   "metadata": {},
   "source": [
    "### Address Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0837f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f687ac",
   "metadata": {},
   "source": [
    "Confirming that all observations are in the USA so we can infact ignore this featue in the future.\n",
    "\n",
    "ACTION DECISION: Remove column/feature as part of data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de64199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_USELESS_.extend(['Country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c931f498",
   "metadata": {},
   "source": [
    "#### State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee92495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for bad strings\n",
    "unclean = data['State'].str.startswith(' ').sum() + data['State'].str.endswith(' ').sum()\n",
    "unclean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4280b6",
   "metadata": {},
   "source": [
    "There are none to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b832e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['State'].value_counts(dropna=False,normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c6dcd9",
   "metadata": {},
   "source": [
    "California seems very high even adjusting for population and area size.  It appears we do not have data from the territories like Puerto Rico (PR), US Virgin Islands (VI) but we do have data from District of Columbia (DC).Comments & Actions.  Concerns about balance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af0247",
   "metadata": {},
   "source": [
    "#### City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same Double Check\n",
    "unclean = data['City'].str.startswith(' ').sum() + data['City'].str.endswith(' ').sum()\n",
    "unclean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308c85a",
   "metadata": {},
   "source": [
    "There are no bad string to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54063b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['City'].value_counts(dropna=False,normalize=True).sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcbb3ae",
   "metadata": {},
   "source": [
    "No concerns with balance (or over-representation).  Interesting to see some of these cities make it in the top 25."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb7c62d",
   "metadata": {},
   "source": [
    "#### Zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c8d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "unclean = data['Zipcode'].str.startswith(' ').sum() + data['Zipcode'].str.endswith(' ').sum()\n",
    "unclean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e475afe",
   "metadata": {},
   "source": [
    "No issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8508fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Zipcode'].value_counts(dropna=False,normalize=True).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c6e3b",
   "metadata": {},
   "source": [
    "Like with city, there doesn't appear to be any concern with the balance.  The syntax with the 4-digit extension (used to tell delivery system a more specific location) should probably be removed since we want consistency.\n",
    "\n",
    "ACTION DECISION: Force all zipcodes to follow simple five digit syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Zipcode'].str.contains(r'[0-9]{5}').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa40854",
   "metadata": {},
   "source": [
    "#### Airport Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "unclean = data['Airport_Code'].str.startswith(' ').sum() + data['Airport_Code'].str.endswith(' ').sum()\n",
    "unclean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af77c1b",
   "metadata": {},
   "source": [
    "No issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b03491",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Airport_Code'].value_counts(dropna=False,normalize=True).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c04b6",
   "metadata": {},
   "source": [
    "The distribution itslef isn't necessarily a concern. However, these codes do not make much sense.  If KCQT is actually a code for an airport, it would be refering to Seattle (WA) and KRDU would be refering to the airport in Raleigh-Durham (NC).  The latter especialy would be surprising from common sense.  Where is LAX?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lax_count = data[data['Airport_Code']=='LAX'].shape[0]\n",
    "print(f'There are {lax_count} incidents with LAX as its airport code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1195a7",
   "metadata": {},
   "source": [
    "Upon further review, this may be indication that this feature is not the traditional airport codes but weather stations!  If that is the case, this feature would have limited analytical value (for now).\n",
    "\n",
    "ACTION DECISION: Remove this feature from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dae4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_USELESS_.extend(['Airport_Code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a812c4",
   "metadata": {},
   "source": [
    "#### Timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ebc8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Timezone'].value_counts(normalize=True,dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbde136",
   "metadata": {},
   "source": [
    "No information can be gleaned from this feature.  I will be grouping the states later on but in ways that either make sense for analytics or for \"real-life\" recommendations.\n",
    "\n",
    "ACTION DECISION: Remove this feature from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eebb97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_USELESS_.extend(['Timezone'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aac600",
   "metadata": {},
   "source": [
    "#### Street Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ee250",
   "metadata": {},
   "outputs": [],
   "source": [
    "unclean = data['Street'].str.startswith(' ').sum() + data['Street'].str.endswith(' ').sum()\n",
    "unclean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c7131",
   "metadata": {},
   "source": [
    "Actually need to handle this if we are to use this feature and need to make sure it is handled in data processing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263bb52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "streets = data['Street'].str.strip()\n",
    "unclean = streets.str.startswith(' ').sum() + streets.str.endswith(' ').sum()\n",
    "unclean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b16c91",
   "metadata": {},
   "source": [
    "Simple fix.\n",
    "\n",
    "ACTION DECISION: Strip the string feature in data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459438f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "streets.value_counts(dropna=False,normalize=True).sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b1935",
   "metadata": {},
   "source": [
    "Not surprising.  We may want to use this information in feature engineering below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5919b373",
   "metadata": {},
   "source": [
    "Need to figure out if there is a consistent syntax for street address."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883de648",
   "metadata": {},
   "source": [
    "We are going to use the following resources to consider if we can group these observations based on the street name where the accident is reported to have occured.\n",
    "\n",
    "__[Google](https://www.google.com/search?q=us+highways+and+interstates&client=safari&sca_esv=a2d3658fd380a756&rls=en&ei=t65JaN2uMMKh5NoPo6mVkQo&oq=US+highways+vs%C2%A0&gs_lp=Egxnd3Mtd2l6LXNlcnAaAhgCIhBVUyBoaWdod2F5cyB2c8KgKgIIADILEAAYgAQYkQIYigUyBxAAGIAEGAoyBRAAGIAEMgYQABgWGB4yBhAAGBYYHjIGEAAYFhgeMgsQABiABBiGAxiKBTILEAAYgAQYhgMYigUyCxAAGIAEGIYDGIoFMggQABiABBiiBEiDTFDVDFjWP3AFeAGQAQCYAXOgAdcKqgEEMTYuMbgBAcgBAPgBAZgCFqACgwzCAgoQABiwAxjWBBhHwgIKEAAYgAQYQxiKBcICERAuGIAEGLEDGNEDGIMBGMcBwgILEAAYgAQYsQMYgwHCAgUQLhiABMICDhAuGIAEGLEDGIMBGNQCwgILEAAYgAQYsQMYiwPCAggQABiABBiLA8ICCBAAGIAEGLEDwgIKEC4YgAQYQxiKBcICERAAGIAEGJECGLEDGIMBGIoFwgIEEAAYA8ICBRAhGKABmAMAiAYBkAYIkgcEMjEuMaAHl26yBwQxNi4xuAfmC8IHBjItMTYuNsgHhwE&sclient=gws-wiz-serp)__ on the difference between US numbered highways and US interstate highway system\n",
    "\n",
    "__[More granular website](https://99percentinvisible.org/article/beyond-streets-avenues-simple-visual-guide-different-types-roads/)__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb9bec",
   "metadata": {},
   "source": [
    "Federal Roads.  Highways & Interstates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "highways = streets.str.startswith('US').sum()+ streets.str.startswith('United States').sum()\n",
    "\n",
    "interstates = streets.str.startswith('I-').sum() + streets.str.startswith('Interstate').sum()\n",
    "\n",
    "fed_roads = highways + interstates\n",
    "\n",
    "msg = f'There are roughly {fed_roads} incidents which occured on a federal roads:  '\n",
    "msg += f'{highways} on a numbered highways and {interstates} interstates.'\n",
    "\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a93e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "highways = streets[streets.str.contains('Highway',na=False)]\n",
    "highways.value_counts(dropna=False,normalize=True).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b442525",
   "metadata": {},
   "source": [
    "Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85943809",
   "metadata": {},
   "outputs": [],
   "source": [
    "routes = streets[streets.str.startswith('Rt',na=False) | streets.str.startswith('Route',na=False)]\n",
    "print(routes.shape[0])\n",
    "routes.value_counts(dropna=False,normalize=True).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b3e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "routes = streets.str.startswith('Route').sum() + streets.str.startswith('Rt').sum()\n",
    "routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901f22e",
   "metadata": {},
   "source": [
    "### Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a7b438",
   "metadata": {},
   "source": [
    "#### Quantitative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f83d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_factors = ['Temperature(F)','Wind_Chill(F)',\n",
    "                   'Humidity(%)','Pressure(in)',\n",
    "                   'Visibility(mi)','Wind_Speed(mph)',\n",
    "                   'Precipitation(in)']\n",
    "\n",
    "data[weather_factors].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88186e45",
   "metadata": {},
   "source": [
    "#### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c65ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_descriptions = data['Weather_Condition'].unique()\n",
    "print(f'There are {weather_descriptions.shape[0]} ways that the weather is described in this dataset: \\n')\n",
    "print(weather_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3135046",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_desc_counts = data['Weather_Condition'].value_counts(dropna=False,normalize=True)\n",
    "weather_desc_counts.sort_values(ascending=False).head(25).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe2dab2",
   "metadata": {},
   "source": [
    "This weather condition feature is too granular to be of any analytical value.  We need to consider consolidating this feature or ignore it all together.  May be of interest to consider how these categories relate to the numeric weather features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a3c2d",
   "metadata": {},
   "source": [
    "### Night/Day Features\n",
    "\n",
    "There appears to be multiple indicators for day/night that need to be explored.  According to __[Google](https://www.google.com/search?q=different+types+of+twilight&rlz=1C5OZZY_enUS1127US1127&oq=different+types+of+twilight&gs_lcrp=EgZjaHJvbWUyCQgAEEUYORiABDIICAEQABgWGB4yCAgCEAAYFhgeMggIAxAAGBYYHjIICAQQABgWGB4yDQgFEAAYhgMYgAQYigUyDQgGEAAYhgMYgAQYigUyDQgHEAAYhgMYgAQYigUyDQgIEAAYhgMYgAQYigUyCggJEAAYgAQYogTSAQg0NjU3ajFqN6gCALACAA&sourceid=chrome&ie=UTF-8)__, Nautical, Astronomical and Civil all have precise definitions--based on usage--that do not, to me, inform as to how their distinctions contribute to understanding accident patterns.  And as you can see below, there is no clear pattern in how they differ from one another in classifying day vs night.\n",
    "\n",
    "Ultimately, I think dropping all of them except for Sunrise_Sunset is for the best.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac78bb",
   "metadata": {},
   "source": [
    "### Night/Day Features\n",
    "\n",
    "There appears to be multiple indicators for day/night that need to be explored.  According to __[Google](https://www.google.com/search?q=different+types+of+twilight&rlz=1C5OZZY_enUS1127US1127&oq=different+types+of+twilight&gs_lcrp=EgZjaHJvbWUyCQgAEEUYORiABDIICAEQABgWGB4yCAgCEAAYFhgeMggIAxAAGBYYHjIICAQQABgWGB4yDQgFEAAYhgMYgAQYigUyDQgGEAAYhgMYgAQYigUyDQgHEAAYhgMYgAQYigUyDQgIEAAYhgMYgAQYigUyCggJEAAYgAQYogTSAQg0NjU3ajFqN6gCALACAA&sourceid=chrome&ie=UTF-8)__, Nautical, Astronomical and Civil all have precise definitions--based on usage--that do not, to me, inform as to how their distinctions contribute to understanding accident patterns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1280e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.crosstab(data['Nautical_Twilight'],data['Astronomical_Twilight']))\n",
    "print('-'*50)\n",
    "print(pd.crosstab(data['Nautical_Twilight'],data['Civil_Twilight']))\n",
    "print('-'*50)\n",
    "print(pd.crosstab(data['Nautical_Twilight'],data['Sunrise_Sunset']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231fc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.crosstab(data['Astronomical_Twilight'],data['Civil_Twilight']))\n",
    "print('-'*50)\n",
    "print(pd.crosstab(data['Astronomical_Twilight'],data['Sunrise_Sunset']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.crosstab(data['Civil_Twilight'],data['Sunrise_Sunset']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57800c2d",
   "metadata": {},
   "source": [
    " And as you can see, there is no clear pattern in how they differ from one another in classifying day vs night.\n",
    "\n",
    "Ultimately, I think dropping all of them except for Sunrise_Sunset is for the best.\n",
    "\n",
    "ACTION DECISION: Remove Nautical_Twilight, Civil_Twilight, Astronomical_Twilight and leave Sunrise_Sunse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eee88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_USELESS_.extend(['Nautical_Twilight','Civil_Twilight','Astronomical_Twilight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528222ee",
   "metadata": {},
   "source": [
    "### Infrastructure\n",
    "Twelve binary features indicating the existance of a traffic design or a PoI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "infrastructures = ['Amenity','Bump','Crossing','Give_Way',\n",
    "                   'Junction','No_Exit','Railway','Roundabout',\n",
    "                   'Station','Stop','Traffic_Calming','Traffic_Signal',\n",
    "                   'Turning_Loop'\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868c4cb7",
   "metadata": {},
   "source": [
    "Need to verify what the definitions of each of these are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8971c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[infrastructures].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366b2649",
   "metadata": {},
   "source": [
    "How often do these overlap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e380b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[infrastructures].sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data[infrastructures].corr(),vmin=-1,vmax=1,cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dccfcce",
   "metadata": {},
   "source": [
    "# 3-Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36995c4c",
   "metadata": {},
   "source": [
    "Start with a brand new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8477fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "if _SPARK_:\n",
    "    spark.stop()\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489df642",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    spark = SparkSession.builder.appName(\"Accident Data Project\").getOrCreate()\n",
    "    data_clean = spark.read.csv('US_Accidents_March23.csv',header=True,inferSchema=True)\n",
    "else:\n",
    "    if _LITE_SWITCH_:\n",
    "        data_clean = pd.read_csv('US_Accidents_March23_sampled_500k.csv')\n",
    "    else:\n",
    "        data_clean = pd.read_csv('US_Accidents_March23.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c580e",
   "metadata": {},
   "source": [
    "## Drop Unnecessary Columns\n",
    "Based on prior exploratory work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbaa9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(_USELESS_))\n",
    "_USELESS_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean.drop(_USELESS_,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cf909f",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03271fa9",
   "metadata": {},
   "source": [
    "### End Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055c4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['End_Lat'] = data_clean['End_Lat'].fillna(data_clean['Start_Lat'])\n",
    "data_clean['End_Lng'] = data_clean['End_Lng'].fillna(data_clean['Start_Lng'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d1d22",
   "metadata": {},
   "source": [
    "### Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0520a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_missing = ['Weather_Condition','Temperature(F)','Pressure(in)','Visibility(mi)','Humidity(%)']\n",
    "data_clean = data_clean.dropna(subset = drop_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5ec0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Precipitation(in)'] = data_clean['Precipitation(in)'].fillna(0)\n",
    "data_clean['Wind_Speed(mph)'] = data_clean['Wind_Speed(mph)'].fillna(0)\n",
    "data_clean['Wind_Direction'] = data_clean['Wind_Direction'].fillna('CALM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e494b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Wind_Chill(F)'] = data_clean['Wind_Chill(F)'].fillna(data_clean['Temperature(F)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed2ad8",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08922e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35ec1a24",
   "metadata": {},
   "source": [
    "### Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f57c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a95af62",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22655f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = data_clean['Distance(mi)'] > 100\n",
    "data_clean = data_clean[condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9713ef9",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f8dec",
   "metadata": {},
   "source": [
    "### Impact Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdcf674",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    pass\n",
    "else:\n",
    "    data_clean['Start'] = pd.to_datetime(data_clean['Start_Time'],format='mixed')\n",
    "    data_clean['End'] = pd.to_datetime(data_clean['End_Time'],format='mixed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad986913",
   "metadata": {},
   "source": [
    "And create a new target feature describing the nature of the accident as a function of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Time_of_Impact(hr)'] = (data_clean['End'] - data_clean['Start']).dt.seconds/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171876b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Time_of_Impact(hr)'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ae531",
   "metadata": {},
   "source": [
    "### Partition the time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be829af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    pass\n",
    "else:\n",
    "    data_clean['Start'] = pd.to_datetime(data_clean['Start_Time'],format='mixed')\n",
    "    data_clean['End'] = pd.to_datetime(data_clean['End_Time'],format='mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741949f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    pass\n",
    "else:\n",
    "    data_clean['Month'] = data_clean['Start'].dt.month\n",
    "    data_clean['Year'] = data_clean['Start'].dt.year\n",
    "    data_clean['DayofYear'] = data_clean['Start'].dt.dayofyear\n",
    "    data_clean['DayofMonth'] = data_clean['Start'].dt.day\n",
    "    data_clean['DayofWeek'] = data_clean['Start'].dt.day_of_week\n",
    "    data_clean['Quarter'] = data_clean['Start'].dt.quarter\n",
    "    data_clean['Hour'] = data_clean['Start'].dt.hour\n",
    "    data_clean['Date'] = data_clean['Start'].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adff36c",
   "metadata": {},
   "source": [
    "### Explore these new features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Year'].value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abddec5e",
   "metadata": {},
   "source": [
    "Concering the book end years are so under represented.  Consider removing entirely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd138f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Quarter'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Month'].value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0557eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['DayofWeek'].value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea8e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Hour'].value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b921f4",
   "metadata": {},
   "source": [
    "### Weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda479a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Weekend'] = ((data_clean['DayofWeek'] == 0) | (data_clean['DayofWeek'] == 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f2f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {data_clean['Weekend'].sum()} observations which occured over the weekend.')\n",
    "data_clean.groupby('Weekend')['Severity'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf0bfae",
   "metadata": {},
   "source": [
    "### Holidays\n",
    "Per __[Google](https://www.google.com/search?client=safari&rls=en&q=holidays+with+most+traffic&ie=UTF-8&oe=UTF-8)__, there are a number of holidays in the US with the highest amount of traffic.  (I subbed NYE in for XMas.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b367f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "thanksgivings = [get_thanksgiving_date(y) for y in range(2016,2024)]\n",
    "memorials = [calculate_memorial_day(y) for y in range(2016,2024)]\n",
    "labors = [get_labor_day(y) for y in range(2016,2024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835730bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "july_4 = ((data_clean['Month'] == 7) & (data_clean['DayofMonth'] == 4))\n",
    "thanksgiving = data_clean['Date'].apply(lambda d:d in thanksgivings)\n",
    "memorial = data_clean['Date'].apply(lambda d:d in memorials)\n",
    "labor = data_clean['Date'].apply(lambda d:d in labors)\n",
    "nye = ((data_clean['Month'] == 12) & (data_clean['DayofMonth'] == 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285c167",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = (july_4 | thanksgiving | memorial | labor | nye)\n",
    "data_clean['Holiday'] = holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce398736",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = f'''{holidays.sum()} incidents occured on a prime holiday.'''\n",
    "\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f88b5b",
   "metadata": {},
   "source": [
    "### Rush Hour Indicator\n",
    "\n",
    "__[Per Google:  ](https://www.google.com/search?client=safari&rls=en&q=rush+hour+typically&ie=UTF-8&oe=UTF-8)__ The morning rush hour begins around 6a, peaking between 7a & 9a, and eases off by 10a.  While afternoon/evening rush hour begins around 3p, peaking between 4p and 6p, and eases off by 7p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f0c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Morning_Rush'] = ((6 > data_clean['DayofWeek']) & (data_clean['DayofWeek'] > 0) & (data_clean['Start'].dt.hour >= 6) & (10 >= data_clean['Start'].dt.hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total of {data_clean['Morning_Rush'].sum()} incidents during the morning rush hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e0993",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Evening_Rush'] = ((6 > data_clean['DayofWeek']) & (data_clean['DayofWeek'] > 0) & (data_clean['Start'].dt.hour >= 15) & (19 >= data_clean['Start'].dt.hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e252c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total of {data_clean['Evening_Rush'].sum()} incidents during the evening rush hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc515ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Rush_Hour'] = data_clean['Evening_Rush'] | data_clean['Morning_Rush']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef062e6",
   "metadata": {},
   "source": [
    "### Attach Location-Based Data to DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e965d",
   "metadata": {},
   "source": [
    "Resources\n",
    "\n",
    "https://www.faa.gov/air_traffic/publications/atpubs/cnt_html/appendix_a.html\n",
    "\n",
    "\n",
    "https://apps.bea.gov/regional/geography.htm\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_regions_of_the_United_States\n",
    "\n",
    "https://www2.census.gov/geo/pdfs/reference/GARM/Ch6GARM.pdf\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/Megaregions_of_the_United_States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a8dfe",
   "metadata": {},
   "source": [
    "### Consider Street Type Feature(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd50c8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ba60879",
   "metadata": {},
   "source": [
    "## Saveout Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89644c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LITE_SWITCH_:\n",
    "    data_clean.to_csv('AccidentData_Sampled_Clean.csv')\n",
    "else:\n",
    "    data_clean.to_csv('AccidentData_Clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f065f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_clean\n",
    "if _SPARK_:\n",
    "    spark.stop()\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ce46e6",
   "metadata": {},
   "source": [
    "# Full EDA\n",
    "Now with cleaned up data with additional features, there are still some areas of exploration that I want to cover to help inform A) possible other features and B) the statistical analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd0141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    spark = SparkSession.builder.appName(\"Accident Data Project\").getOrCreate()\n",
    "    data = spark.read.csv('AccidentData_Clean.csv',header=True,inferSchema=True)\n",
    "else:\n",
    "    if _LITE_SWITCH_:\n",
    "        data = pd.read_csv('AccidentData_Sampled_Clean.csv')\n",
    "    else:\n",
    "        data = pd.read_csv('AccidentData_Clean.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c808c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    data.printSchema()\n",
    "    print(\"Features: \",len(data.columns))\n",
    "    print(\"Entries:  \",data.count())\n",
    "else:\n",
    "    data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e015737",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TARGET_ = ['Severity']\n",
    "_NUMERICS_ = ['Distance(mi)','Time_of_Impact(hr)','Temperature(F)',\n",
    "              'Wind_Chill(F)','Humidity(%)','Pressure(in)',\n",
    "              'Visibility(mi)','Wind_Speed(mph)','Precipitation(in)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377ae790",
   "metadata": {},
   "source": [
    "## Nature of Incident Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89796be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48585451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b01fbb7b",
   "metadata": {},
   "source": [
    "## Relationships between Continous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    corr_data = data.select(_NUMERICS_+['Severity'])\n",
    "    col_names = corr_data.columns\n",
    "    features = corr_data.rdd.map(lambda row: row[0:]) \n",
    "    corrs = Statistics.corr(features, method=\"pearson\")\n",
    "else:\n",
    "    corrs = data[_NUMERICS_].corr()\n",
    "    print(corrs)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corrs,vmin=-1,vmax=1,cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa186c2b",
   "metadata": {},
   "source": [
    "Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62294b57",
   "metadata": {},
   "source": [
    "## Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ddadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    accdidents_year = data.stat.crosstab('Year','Severity')\n",
    "else:\n",
    "    accdidents_year = pd.crosstab(data['Year'],data['Severity'])\n",
    "\n",
    "    accdidents_year['Total']=accdidents_year.sum(axis=1)\n",
    "    accdidents_year['Average']=accdidents_year.apply(lambda r:sum(i*r[i] for i in range(1,5))/r['Total'],axis=1)\n",
    "    accidents_calendar = pd.pivot_table(data,columns='Year',index='Month',values='Severity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(accdidents_year)\n",
    "print('*'*75)\n",
    "display(accidents_calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a595d0",
   "metadata": {},
   "source": [
    "Comments & Concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda5a22",
   "metadata": {},
   "source": [
    "Other Cross Tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78572a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea509841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf570158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f08bf686",
   "metadata": {},
   "source": [
    "### Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36779971",
   "metadata": {},
   "outputs": [],
   "source": [
    "infrastructure_count = pd.DataFrame(columns=infrastructures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c42649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852e15d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4c67706",
   "metadata": {},
   "source": [
    "### Weather Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d289da",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_descriptions = data.Weather_Condition.unique()\n",
    "print(f'There are {weather_descriptions.shape[0]} ways that the weather is described in this dataset: \\n')\n",
    "print(weather_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be12dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_counts = data.Weather_Condition.value_counts(dropna=False,normalize=True)\n",
    "description_counts.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1eebe",
   "metadata": {},
   "source": [
    "To use this feature, we need to consolidate somehow.  But first..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666ce4ba",
   "metadata": {},
   "source": [
    "### Exploration of Weather Condition Consolidation\n",
    "\n",
    "Is there a way, we can use the continous features to create a consolidated but meaningful feature which maybe can improve on the categorical weather feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20515e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_factors = ['Temperature(F)','Wind_Chill(F)','Humidity(%)',\n",
    "                   'Pressure(in)','Visibility(mi)', 'Wind_Direction',\n",
    "                   'Wind_Speed(mph)','Precipitation(in)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d932e2e2",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ad02dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[weather_factors].dropna()\n",
    "scaler = StandardScaler() \n",
    "X_std =scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_std);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b25a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "# Create a cumulative variance plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "cum_var_plot = plt.plot(range(1, len(cumulative_var) + 1), cumulative_var, \n",
    "         'o-', linewidth=2, color='green')\n",
    "# Add lines for 90% and 95% thresholds\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='90% threshold')\n",
    "plt.axhline(y=0.95, color='g', linestyle='--', label='95% threshold')\n",
    "plt.title('Cumulative Variance Explained')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3680aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variance = 0.95\n",
    "n_components_variance = np.argmax(cumulative_var >= target_variance) + 1\n",
    "print(f'We choose to include the first {n_components_variance} components.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f24770",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_components_variance)\n",
    "pca.fit(X_std);\n",
    "# Transform the standardized data to get principal components\n",
    "X_pca = pca.transform(X_std)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "pca_df = pd.DataFrame(\n",
    "    data=X_pca,\n",
    "    columns=[f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
    ")\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc4642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an easy to view DF of the loadings.\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(n_components_variance)],\n",
    "    index=weather_factors\n",
    ")\n",
    "loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb612d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loadings for the first two PCs\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, feature in enumerate(weather_factors):\n",
    "    plt.arrow(0, 0, loadings.iloc[i, 0], loadings.iloc[i, 1], head_width=0.05, head_length=0.05)\n",
    "    plt.text(loadings.iloc[i, 0]*1.1, loadings.iloc[i, 1]*1.1, feature, fontsize=12)\n",
    "\n",
    "# Add a unit circle for reference\n",
    "circle = plt.Circle((0, 0), 1, fill=False, linestyle='--')\n",
    "plt.gca().add_patch(circle)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.xlim(-1.1, 1.1)\n",
    "plt.ylim(-1.1, 1.1)\n",
    "plt.title('PCA Loading Plot (PC1 vs PC2)')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance explained)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance explained)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6a39eb",
   "metadata": {},
   "source": [
    "#### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac1bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[weather_factors+['Weather_Condition']].dropna()[weather_factors]\n",
    "scaler = StandardScaler() \n",
    "X_std =scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e28a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "km = KMeans(random_state=42)\n",
    "visualizer = KElbowVisualizer(km,k=(5,50))\n",
    "visualizer.fit(X_std)\n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b582de8",
   "metadata": {},
   "source": [
    "Recall, the heavily-skewed distribution of weather descripitions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d93d177",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(random_state=42,n_clusters=visualizer.elbow_value_)\n",
    "km.fit(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe3799",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = pd.DataFrame(scaler.inverse_transform(km.cluster_centers_),columns=weather_factors)\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeacbceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(km.labels_).value_counts(normalize=True).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ed7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "described_obsv = pd.DataFrame(data[weather_factors+['Weather_Condition']].dropna()['Weather_Condition'])\n",
    "described_obsv['Cluster'] = km.labels_\n",
    "described_obsv['Severity'] = data['Severity']\n",
    "described_obsv['Distance(mi)'] = data['Distance(mi)']\n",
    "described_obsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa20688",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_v_descr = pd.crosstab(described_obsv['Cluster'],described_obsv['Weather_Condition'])\n",
    "cluster_v_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e835d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_v_sever = pd.crosstab(described_obsv['Cluster'],described_obsv['Severity'])\n",
    "cluster_v_sever['Average'] = cluster_v_sever.apply(lambda r:sum(r[c]*c for c in range(1,5))/sum(r),axis=1)\n",
    "cluster_v_sever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c35d4d",
   "metadata": {},
   "source": [
    "#### Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6459df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65a5a8bf",
   "metadata": {},
   "source": [
    "### Final Saveout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f691fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LITE_SWITCH_:\n",
    "    data_clean.to_csv('AccidentData_Sampled_Clean_2.csv')\n",
    "else:\n",
    "    data_clean.to_csv('AccidentData_Clean_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_clean\n",
    "if _SPARK_:\n",
    "    spark.stop()\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53470b30",
   "metadata": {},
   "source": [
    "# Statistical Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91f1c0b",
   "metadata": {},
   "source": [
    "Description of the three primary tests that are going to be run for all of these categorical features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad966c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    spark = SparkSession.builder.appName(\"Accident Data Project\").getOrCreate()\n",
    "    data = spark.read.csv('AccidentData_Clean_2.csv',header=True,inferSchema=True)\n",
    "else:\n",
    "    if _LITE_SWITCH_:\n",
    "        data = pd.read_csv('AccidentData_Sampled_Clean_2.csv')\n",
    "    else:\n",
    "        data = pd.read_csv('AccidentData_Clean_2.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cdbd9b",
   "metadata": {},
   "source": [
    "## Nature-of-Accident Features vs. Each Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1860a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_distVseverity = ols(\"Q('Distance(mi)') ~ C(Severity)\", data=data).fit()\n",
    "anova_table_distVSeverity = sm.stats.anova_lm(model_distVseverity, typ=2)\n",
    "print(\"\\nANOVA for Distance vs Severity Category:\\n\", anova_table_distVSeverity)\n",
    "p_val = anova_table_distVSeverity['PR(>F)'].iloc[0]\n",
    "print(f\"P-value for Distance vs Severity Category: {p_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aad348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_timeVseverity = ols(\"Q('Time_of_Impact(hr)') ~ C(Severity)\", data=data).fit()\n",
    "anova_table_timeVseverity = sm.stats.anova_lm(model_timeVseverity, typ=2)\n",
    "print(\"\\nANOVA for Time of Impact vs Severity Category:\\n\", anova_table_timeVseverity)\n",
    "p_val = anova_table_timeVseverity['PR(>F)'].iloc[0]\n",
    "print(f\"P-value for Time of Impact vs Severity Category: {p_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6efa284",
   "metadata": {},
   "source": [
    "Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1ccd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_distVseverity, model_timeVseverity\n",
    "del anova_table_distVSeverity, anova_table_timeVseverity\n",
    "del p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967507ef",
   "metadata": {},
   "source": [
    "## Weather-Related\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4284578",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    spark = SparkSession.builder.appName(\"Accident Data Project\").getOrCreate()\n",
    "    data = spark.read.csv('AccidentData_Clean_2.csv',header=True,inferSchema=True)\n",
    "else:\n",
    "    if _LITE_SWITCH_:\n",
    "        data = pd.read_csv('AccidentData_Sampled_Clean_2.csv')\n",
    "    else:\n",
    "        data = pd.read_csv('AccidentData_Clean_2.csv',usecols = ['Weather_Condition','Distance(mi)','Time_of_Impact(hr)'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cde95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "907b5403",
   "metadata": {},
   "source": [
    "## Calendar-Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    spark = SparkSession.builder.appName(\"Accident Data Project\").getOrCreate()\n",
    "    data = spark.read.csv('AccidentData_Clean_2.csv',header=True,inferSchema=True)\n",
    "else:\n",
    "    if _LITE_SWITCH_:\n",
    "        data = pd.read_csv('AccidentData_Sampled_Clean_2.csv')\n",
    "    else:\n",
    "        tgt_cols = ['Month','Quarter','Year','DayofWeek','Weekend','Holiday']\n",
    "        data = pd.read_csv('AccidentData_Clean_2.csv',usecols = tgt_cols+['Severity','Distance(mi)','Time_of_Impact(hr)'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7acd289",
   "metadata": {},
   "source": [
    "### YoY Difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports,stats = test_results('Year',data)\n",
    "data.groupby('Year')[['Severity','Time_of_Impact(hr)','Distance(mi)']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e43678",
   "metadata": {},
   "source": [
    "### Quarterly Difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d232d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports,stats = test_results('Quarter',data)\n",
    "data.groupby('Quarter')[['Severity','Time_of_Impact(hr)','Distance(mi)']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9877e9ba",
   "metadata": {},
   "source": [
    "### Monthly ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d29cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports,stats = test_results('Month',data)\n",
    "data.groupby('Month')[['Severity','Time_of_Impact(hr)','Distance(mi)']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfb04d1",
   "metadata": {},
   "source": [
    "### Day of Week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports,stats = test_results('DayofWeek',data)\n",
    "data.groupby('DayofWeek')[['Severity','Time_of_Impact(hr)','Distance(mi)']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed2cae3",
   "metadata": {},
   "source": [
    "### Weekends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496462de",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports,stats = test_results('Weekend',data)\n",
    "data.groupby('Weekend')[['Severity','Time_of_Impact(hr)','Distance(mi)']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47ef969",
   "metadata": {},
   "source": [
    "### Holidays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19af187",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports,stats = test_results('Holiday',data)\n",
    "data.groupby('Holiday')[['Severity','Time_of_Impact(hr)','Distance(mi)']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de574ce9",
   "metadata": {},
   "source": [
    "## Time of Day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd713d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    spark = SparkSession.builder.appName(\"Accident Data Project\").getOrCreate()\n",
    "    data = spark.read.csv('AccidentData_Clean.csv',header=True,inferSchema=True)\n",
    "else:\n",
    "    if _LITE_SWITCH_:\n",
    "        data = pd.read_csv('AccidentData_Sampled_Clean.csv')\n",
    "    else:\n",
    "        tgt_cols = ['Hour','Morning_Rush','Evening_Rush','Rush_Hour','Sunrise_Sunset']\n",
    "        data = pd.read_csv('AccidentData_Clean.csv',usecols = tgt_cols+['Severity','Distance(mi)','Time_of_Impact(hr)'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f665973",
   "metadata": {},
   "source": [
    "### Hourly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f3221",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports,stats = test_results('Hour',data)\n",
    "data.groupby('Hour')[['Severity','Time_of_Impact(hr)','Distance(mi)']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9883b",
   "metadata": {},
   "source": [
    "Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf079d",
   "metadata": {},
   "source": [
    "### Rush Hours?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b35ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports,stats = test_results('Evening_Rush',data)\n",
    "data.groupby('Evening_Rush')[['Severity','Time_of_Impact(hr)','Distance(mi)']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports,stats = test_results('Morning_Rush',data)\n",
    "data.groupby('Morning_Rush')[['Severity','Time_of_Impact(hr)','Distance(mi)']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdbd53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports,stats = test_results('Rush_Hour',data)\n",
    "data.groupby('Rush_Hour')[['Severity','Time_of_Impact(hr)','Distance(mi)']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de79850",
   "metadata": {},
   "source": [
    "### Day/Night?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5fa1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports,stats = test_results('Sunrise_Sunset',data)\n",
    "data.groupby('Sunrise_Sunset')[['Severity','Time_of_Impact(hr)','Distance(mi)']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79eea1c",
   "metadata": {},
   "source": [
    "## Location-Related"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb23721",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b02ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LITE_SWITCH_:\n",
    "    pass\n",
    "elif _SPARK_:\n",
    "    pass\n",
    "else:\n",
    "    try:\n",
    "        del data\n",
    "        del reports, stats\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca6b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    spark = SparkSession.builder.appName(\"Accident Data Project\").getOrCreate()\n",
    "    data = spark.read.csv('AccidentData_Clean.csv',header=True,inferSchema=True)\n",
    "else:\n",
    "    if _LITE_SWITCH_:\n",
    "        data = pd.read_csv('AccidentData_Sampled_Clean.csv')\n",
    "    else:\n",
    "        tgt_cols = ['State']\n",
    "        data = pd.read_csv('AccidentData_Clean.csv',usecols = tgt_cols + ['Severity','Distance(mi)','Time_of_Impact(hr)'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea62ac",
   "metadata": {},
   "source": [
    "### By State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89af6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = data['State'].unique()\n",
    "\n",
    "for state in states:\n",
    "    print(state + ':')\n",
    "    data['inState'] = data['State'] == state\n",
    "    test_results('inState',data,visuals=False)\n",
    "    print('-'*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef137f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('State')[['Severity','Time_of_Impact(hr)','Distance(mi)']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f767be",
   "metadata": {},
   "source": [
    "### Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa1fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "infrastructures = ['Amenity','Bump','Crossing','Give_Way',\n",
    "                   'Junction','No_Exit','Railway','Roundabout',\n",
    "                   'Station','Stop','Traffic_Calming','Traffic_Signal',\n",
    "                   'Turning_Loop'\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _SPARK_:\n",
    "    spark = SparkSession.builder.appName(\"Accident Data Project\").getOrCreate()\n",
    "    data = spark.read.csv('AccidentData_Clean.csv',header=True,inferSchema=True)\n",
    "else:\n",
    "    if _LITE_SWITCH_:\n",
    "        data = pd.read_csv('AccidentData_Sampled_Clean.csv')\n",
    "    else:\n",
    "        data = pd.read_csv('AccidentData_Clean.csv',usecols = infrastructures + ['Severity','Distance(mi)','Time_of_Impact(hr)'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b28e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for structure in infrastructures:\n",
    "    print(structure+\": \")\n",
    "    reports,stats = test_results(structure,data,visuals=True)\n",
    "    display(data.groupby(structure)[['Severity','Time_of_Impact(hr)','Distance(mi)']].mean())\n",
    "    print('-'*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed2a5c",
   "metadata": {},
   "source": [
    "# 6-Advanced Analysis (Wish List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b63d5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a2c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0089757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c455d177",
   "metadata": {},
   "source": [
    "# 7-Insights & Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab1a24",
   "metadata": {},
   "source": [
    "## Basic Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40b186",
   "metadata": {},
   "source": [
    "### Annual Distinctions\n",
    "\n",
    "Number of Accidents; Distribution of Severities; Length of Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68751bf8",
   "metadata": {},
   "source": [
    "### Temporal & Spatial Considerations\n",
    "\n",
    "How do accident counts relate to different times of the day and for different region types (urban vs rural)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b11f0be",
   "metadata": {},
   "source": [
    "### Weather Considerations\n",
    "Does certain weather conditions produce more severe incidents?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f4256",
   "metadata": {},
   "source": [
    "# Advanced (Wish List) Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7394e4c",
   "metadata": {},
   "source": [
    "## Unusual Weather \n",
    "\n",
    "Does driving in unexpected weather--based on area, time of year and/or both--create a higher likelihood of an accident."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c08b1a",
   "metadata": {},
   "source": [
    "## Famous Highways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aeafa3",
   "metadata": {},
   "source": [
    "## Highway's Near Urban Areas\n",
    "\n",
    "For cross-state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c98d881",
   "metadata": {},
   "source": [
    "## Naturual Language\n",
    "\n",
    "Examination of the description feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b44906",
   "metadata": {},
   "source": [
    "## Safety Infrastructure\n",
    "Does certain road infrastructure projects help reduce the number of incidents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da82e791",
   "metadata": {},
   "source": [
    "## New Traffic Pattern\n",
    "\n",
    "Does the existence of a new traffic pattern in the area increase the likelihood of an accident?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90418e",
   "metadata": {},
   "source": [
    "## Recent Accident Indicator\n",
    "\n",
    "Does the presence of one accident, predict another."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
